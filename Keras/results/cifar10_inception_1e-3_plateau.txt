# Training parameters
LEARNING_RATE = 1e-3
DATA_AUGMENTATION = True
BATCH_SIZE = 128
EPOCHS = 200
KERNEL_INITIALIZER = 'he_normal'
es = keras.callbacks.EarlyStopping(monitor='val_acc', patience=15)
reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=1)


Using TensorFlow backend.
Using data augmentation.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0
__________________________________________________________________________________________________
stage1_conv1 (Conv2D)           (None, 32, 32, 32)   896         input_1[0][0]
__________________________________________________________________________________________________
stage1_conv2 (Conv2D)           (None, 32, 32, 32)   9248        stage1_conv1[0][0]
__________________________________________________________________________________________________
stage1_maxpool (MaxPooling2D)   (None, 16, 16, 32)   0           stage1_conv2[0][0]
__________________________________________________________________________________________________
stage2_1_1 (Conv2D)             (None, 16, 16, 64)   18496       stage1_maxpool[0][0]
__________________________________________________________________________________________________
stage2_3_3 (Conv2D)             (None, 16, 16, 128)  36992       stage1_maxpool[0][0]
__________________________________________________________________________________________________
stage2_5_5 (Conv2D)             (None, 16, 16, 32)   9248        stage1_maxpool[0][0]
__________________________________________________________________________________________________
stage2_pool_1_1 (Conv2D)        (None, 16, 16, 32)   9248        stage1_maxpool[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 16, 16, 256)  0           stage2_1_1[0][0]
                                                                 stage2_3_3[0][0]
                                                                 stage2_5_5[0][0]
                                                                 stage2_pool_1_1[0][0]
__________________________________________________________________________________________________
stage2_maxpool (MaxPooling2D)   (None, 8, 8, 256)    0           concatenate_1[0][0]
__________________________________________________________________________________________________
stage3_1_1 (Conv2D)             (None, 8, 8, 128)    295040      stage2_maxpool[0][0]
__________________________________________________________________________________________________
stage3_3_3 (Conv2D)             (None, 8, 8, 192)    442560      stage2_maxpool[0][0]
__________________________________________________________________________________________________
stage3_5_5 (Conv2D)             (None, 8, 8, 96)     221280      stage2_maxpool[0][0]
__________________________________________________________________________________________________
stage3_pool_1_1 (Conv2D)        (None, 8, 8, 64)     147520      stage2_maxpool[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 480)    0           stage3_1_1[0][0]
                                                                 stage3_3_3[0][0]
                                                                 stage3_5_5[0][0]
                                                                 stage3_pool_1_1[0][0]
__________________________________________________________________________________________________
stage3_maxpool (MaxPooling2D)   (None, 4, 4, 480)    0           concatenate_2[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 480)    0           stage3_maxpool[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 480)          0           average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 480)          0           flatten_1[0][0]
__________________________________________________________________________________________________
stage4_dense1 (Dense)           (None, 10)           4810        dropout_1[0][0]
==================================================================================================
Total params: 1,195,338
Trainable params: 1,195,338
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/200
391/391 [==============================] - 17s 43ms/step - loss: 1.7723 - acc: 0.3420 - val_loss: 1.6469 - val_acc: 0.4150
Epoch 2/200
391/391 [==============================] - 14s 37ms/step - loss: 1.4450 - acc: 0.4808 - val_loss: 1.2834 - val_acc: 0.5438
Epoch 3/200
391/391 [==============================] - 14s 36ms/step - loss: 1.2874 - acc: 0.5419 - val_loss: 1.1149 - val_acc: 0.6120
Epoch 4/200
391/391 [==============================] - 14s 36ms/step - loss: 1.1692 - acc: 0.5870 - val_loss: 1.0837 - val_acc: 0.6259
Epoch 5/200
391/391 [==============================] - 14s 36ms/step - loss: 1.0763 - acc: 0.6229 - val_loss: 1.1412 - val_acc: 0.6305
Epoch 6/200
391/391 [==============================] - 14s 36ms/step - loss: 1.0116 - acc: 0.6460 - val_loss: 0.8737 - val_acc: 0.6967
Epoch 7/200
391/391 [==============================] - 14s 36ms/step - loss: 0.9575 - acc: 0.6685 - val_loss: 0.8572 - val_acc: 0.7081
Epoch 8/200
391/391 [==============================] - 14s 36ms/step - loss: 0.9095 - acc: 0.6846 - val_loss: 0.8267 - val_acc: 0.7261
Epoch 9/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8623 - acc: 0.7033 - val_loss: 0.8371 - val_acc: 0.7225
Epoch 10/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8260 - acc: 0.7171 - val_loss: 0.8332 - val_acc: 0.7299
Epoch 11/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7964 - acc: 0.7241 - val_loss: 0.6923 - val_acc: 0.7656
Epoch 12/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7662 - acc: 0.7364 - val_loss: 0.7291 - val_acc: 0.7615
Epoch 13/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7495 - acc: 0.7417 - val_loss: 0.6961 - val_acc: 0.7659
Epoch 14/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7180 - acc: 0.7520 - val_loss: 0.6904 - val_acc: 0.7800
Epoch 15/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6976 - acc: 0.7604 - val_loss: 0.6865 - val_acc: 0.7778
Epoch 16/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6894 - acc: 0.7624 - val_loss: 0.6369 - val_acc: 0.7897
Epoch 17/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6691 - acc: 0.7716 - val_loss: 0.6647 - val_acc: 0.7778
Epoch 18/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6527 - acc: 0.7756 - val_loss: 0.6407 - val_acc: 0.7896
Epoch 19/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6395 - acc: 0.7790 - val_loss: 0.5976 - val_acc: 0.8027
Epoch 20/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6255 - acc: 0.7856 - val_loss: 0.5792 - val_acc: 0.8109
Epoch 21/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6112 - acc: 0.7915 - val_loss: 0.6224 - val_acc: 0.7996
Epoch 22/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6025 - acc: 0.7946 - val_loss: 0.6949 - val_acc: 0.7870
Epoch 23/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5932 - acc: 0.7961 - val_loss: 0.5572 - val_acc: 0.8199
Epoch 24/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5819 - acc: 0.7998 - val_loss: 0.6100 - val_acc: 0.8109
Epoch 25/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5661 - acc: 0.8047 - val_loss: 0.5437 - val_acc: 0.8262
Epoch 26/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5720 - acc: 0.8028 - val_loss: 0.5685 - val_acc: 0.8166
Epoch 27/200
391/391 [==============================] - 14s 37ms/step - loss: 0.5564 - acc: 0.8106 - val_loss: 0.5448 - val_acc: 0.8257
Epoch 28/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5488 - acc: 0.8112 - val_loss: 0.5661 - val_acc: 0.8191
Epoch 29/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5372 - acc: 0.8138 - val_loss: 0.5506 - val_acc: 0.8259
Epoch 30/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5313 - acc: 0.8170 - val_loss: 0.5647 - val_acc: 0.8217
Epoch 31/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5242 - acc: 0.8185 - val_loss: 0.5328 - val_acc: 0.8274
Epoch 32/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5135 - acc: 0.8228 - val_loss: 0.5642 - val_acc: 0.8190
Epoch 33/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5099 - acc: 0.8248 - val_loss: 0.5049 - val_acc: 0.8345
Epoch 34/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5076 - acc: 0.8248 - val_loss: 0.5320 - val_acc: 0.8334
Epoch 35/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5000 - acc: 0.8267 - val_loss: 0.5017 - val_acc: 0.8388
Epoch 36/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4987 - acc: 0.8288 - val_loss: 0.5254 - val_acc: 0.8333
Epoch 37/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4856 - acc: 0.8323 - val_loss: 0.5980 - val_acc: 0.8154
Epoch 38/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4820 - acc: 0.8349 - val_loss: 0.5067 - val_acc: 0.8367
Epoch 39/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4778 - acc: 0.8373 - val_loss: 0.4914 - val_acc: 0.8447
Epoch 40/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4711 - acc: 0.8383 - val_loss: 0.5463 - val_acc: 0.8316
Epoch 41/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4661 - acc: 0.8380 - val_loss: 0.4934 - val_acc: 0.8433
Epoch 42/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4657 - acc: 0.8397 - val_loss: 0.4923 - val_acc: 0.8444
Epoch 43/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4626 - acc: 0.8415 - val_loss: 0.5404 - val_acc: 0.8339
Epoch 44/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4533 - acc: 0.8423 - val_loss: 0.4961 - val_acc: 0.8431
Epoch 45/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4452 - acc: 0.8445 - val_loss: 0.5301 - val_acc: 0.8440
Epoch 46/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4526 - acc: 0.8425 - val_loss: 0.4783 - val_acc: 0.8490
Epoch 47/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4401 - acc: 0.8487 - val_loss: 0.5585 - val_acc: 0.8338
Epoch 48/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4348 - acc: 0.8503 - val_loss: 0.4844 - val_acc: 0.8452
Epoch 49/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4418 - acc: 0.8488 - val_loss: 0.5110 - val_acc: 0.8438
Epoch 50/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4241 - acc: 0.8537 - val_loss: 0.5620 - val_acc: 0.8410
Epoch 51/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4294 - acc: 0.8533 - val_loss: 0.5421 - val_acc: 0.8353
Epoch 52/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4261 - acc: 0.8523 - val_loss: 0.5221 - val_acc: 0.8444
Epoch 53/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4229 - acc: 0.8540 - val_loss: 0.5450 - val_acc: 0.8299
Epoch 54/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4178 - acc: 0.8539 - val_loss: 0.4939 - val_acc: 0.8477
Epoch 55/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4178 - acc: 0.8556 - val_loss: 0.5124 - val_acc: 0.8490
Epoch 56/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4086 - acc: 0.8571 - val_loss: 0.4897 - val_acc: 0.8546
Epoch 57/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4094 - acc: 0.8576 - val_loss: 0.5018 - val_acc: 0.8500
Epoch 58/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4037 - acc: 0.8604 - val_loss: 0.5130 - val_acc: 0.8423
Epoch 59/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3975 - acc: 0.8631 - val_loss: 0.4780 - val_acc: 0.8539
Epoch 60/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4012 - acc: 0.8616 - val_loss: 0.5009 - val_acc: 0.8470
Epoch 61/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4009 - acc: 0.8626 - val_loss: 0.4968 - val_acc: 0.8522
Epoch 62/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3979 - acc: 0.8626 - val_loss: 0.5233 - val_acc: 0.8473
Epoch 63/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3944 - acc: 0.8652 - val_loss: 0.4344 - val_acc: 0.8639
Epoch 64/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3903 - acc: 0.8662 - val_loss: 0.4719 - val_acc: 0.8560
Epoch 65/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3905 - acc: 0.8651 - val_loss: 0.4187 - val_acc: 0.8683
Epoch 66/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3918 - acc: 0.8643 - val_loss: 0.4933 - val_acc: 0.8539
Epoch 67/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3875 - acc: 0.8645 - val_loss: 0.5603 - val_acc: 0.8450
Epoch 68/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3778 - acc: 0.8689 - val_loss: 0.5653 - val_acc: 0.8375
Epoch 69/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3762 - acc: 0.8705 - val_loss: 0.4869 - val_acc: 0.8572
Epoch 70/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3814 - acc: 0.8695 - val_loss: 0.4938 - val_acc: 0.8511
Epoch 71/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3702 - acc: 0.8720 - val_loss: 0.5678 - val_acc: 0.8411
Epoch 72/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3708 - acc: 0.8715 - val_loss: 0.4622 - val_acc: 0.8641
Epoch 73/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3711 - acc: 0.8721 - val_loss: 0.4734 - val_acc: 0.8595
Epoch 74/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3702 - acc: 0.8707 - val_loss: 0.4985 - val_acc: 0.8551
Epoch 75/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3632 - acc: 0.8734 - val_loss: 0.5398 - val_acc: 0.8489

Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 76/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3245 - acc: 0.8867 - val_loss: 0.4735 - val_acc: 0.8643
Epoch 77/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3141 - acc: 0.8917 - val_loss: 0.4685 - val_acc: 0.8681
Epoch 78/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3117 - acc: 0.8904 - val_loss: 0.4649 - val_acc: 0.8682
Epoch 79/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3054 - acc: 0.8931 - val_loss: 0.4515 - val_acc: 0.8704
Epoch 80/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3015 - acc: 0.8937 - val_loss: 0.4489 - val_acc: 0.8711
Epoch 81/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3044 - acc: 0.8931 - val_loss: 0.4695 - val_acc: 0.8665
Epoch 82/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2950 - acc: 0.8971 - val_loss: 0.4591 - val_acc: 0.8696
Epoch 83/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2962 - acc: 0.8967 - val_loss: 0.4623 - val_acc: 0.8681
Epoch 84/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2948 - acc: 0.8957 - val_loss: 0.4422 - val_acc: 0.8705
Epoch 85/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2991 - acc: 0.8959 - val_loss: 0.4381 - val_acc: 0.8713
Epoch 86/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3011 - acc: 0.8947 - val_loss: 0.4501 - val_acc: 0.8687
Epoch 87/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2924 - acc: 0.8970 - val_loss: 0.4604 - val_acc: 0.8678
Epoch 88/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2920 - acc: 0.8986 - val_loss: 0.4526 - val_acc: 0.8690
Epoch 89/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2875 - acc: 0.8998 - val_loss: 0.4539 - val_acc: 0.8707
Epoch 90/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2953 - acc: 0.8967 - val_loss: 0.4535 - val_acc: 0.8713
Epoch 91/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2935 - acc: 0.8989 - val_loss: 0.4649 - val_acc: 0.8673
Epoch 92/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2862 - acc: 0.9003 - val_loss: 0.4484 - val_acc: 0.8717
Epoch 93/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2889 - acc: 0.9013 - val_loss: 0.4432 - val_acc: 0.8724
Epoch 94/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2894 - acc: 0.8986 - val_loss: 0.4685 - val_acc: 0.8692
Epoch 95/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2855 - acc: 0.9024 - val_loss: 0.4521 - val_acc: 0.8721
Epoch 96/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2874 - acc: 0.8989 - val_loss: 0.4486 - val_acc: 0.8700
Epoch 97/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2855 - acc: 0.8993 - val_loss: 0.4538 - val_acc: 0.8707
Epoch 98/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2830 - acc: 0.9010 - val_loss: 0.4529 - val_acc: 0.8709
Epoch 99/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2846 - acc: 0.8998 - val_loss: 0.4619 - val_acc: 0.8706
Epoch 100/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2817 - acc: 0.9019 - val_loss: 0.4261 - val_acc: 0.8741
Epoch 101/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2852 - acc: 0.8998 - val_loss: 0.4596 - val_acc: 0.8713
Epoch 102/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2808 - acc: 0.9016 - val_loss: 0.4387 - val_acc: 0.8733
Epoch 103/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2818 - acc: 0.9012 - val_loss: 0.4649 - val_acc: 0.8689
Epoch 104/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2808 - acc: 0.9020 - val_loss: 0.4265 - val_acc: 0.8748
Epoch 105/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2807 - acc: 0.9029 - val_loss: 0.4602 - val_acc: 0.8702
Epoch 106/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2792 - acc: 0.9032 - val_loss: 0.4479 - val_acc: 0.8734
Epoch 107/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2729 - acc: 0.9041 - val_loss: 0.4461 - val_acc: 0.8745
Epoch 108/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2758 - acc: 0.9029 - val_loss: 0.4543 - val_acc: 0.8707
Epoch 109/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2767 - acc: 0.9035 - val_loss: 0.4450 - val_acc: 0.8748
Epoch 110/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2774 - acc: 0.9036 - val_loss: 0.4585 - val_acc: 0.8723
Epoch 111/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2783 - acc: 0.9025 - val_loss: 0.4487 - val_acc: 0.8730
Epoch 112/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2762 - acc: 0.9037 - val_loss: 0.4538 - val_acc: 0.8696
Epoch 113/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2696 - acc: 0.9055 - val_loss: 0.4394 - val_acc: 0.8737
Epoch 114/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2693 - acc: 0.9046 - val_loss: 0.4503 - val_acc: 0.8735

Epoch 00114: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 115/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2743 - acc: 0.9037 - val_loss: 0.4452 - val_acc: 0.8744
Epoch 116/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2692 - acc: 0.9056 - val_loss: 0.4448 - val_acc: 0.8741
Epoch 117/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2698 - acc: 0.9058 - val_loss: 0.4532 - val_acc: 0.8732
Epoch 118/200
391/391 [==============================] - 14s 36ms/step - loss: 0.2710 - acc: 0.9049 - val_loss: 0.4459 - val_acc: 0.8744
Epoch 119/200
391/391 [==============================] - 14s 35ms/step - loss: 0.2659 - acc: 0.9072 - val_loss: 0.4505 - val_acc: 0.8733