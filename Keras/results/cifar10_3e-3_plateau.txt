# Training parameters
LEARNING_RATE = 3e-3
DATA_AUGMENTATION = True
BATCH_SIZE = 128
EPOCHS = 200
BATCH_NORMALIZATION = True
REGULARIZATION = 0.0
KERNEL_INITIALIZER = 'he_normal'
es = keras.callbacks.EarlyStopping(monitor='val_acc', patience=15)
reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=1)




Using TensorFlow backend.
Using data augmentation.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 32, 32, 3)         0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 32, 32, 32)        896
_________________________________________________________________
block1_bn1 (BatchNormalizati (None, 32, 32, 32)        128
_________________________________________________________________
block1_relu1 (Activation)    (None, 32, 32, 32)        0
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 32, 32, 32)        9248
_________________________________________________________________
block1_bn2 (BatchNormalizati (None, 32, 32, 32)        128
_________________________________________________________________
block1_relu2 (Activation)    (None, 32, 32, 32)        0
_________________________________________________________________
block1_maxpool (MaxPooling2D (None, 16, 16, 32)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 16, 16, 64)        18496
_________________________________________________________________
block2_bn1 (BatchNormalizati (None, 16, 16, 64)        256
_________________________________________________________________
block2_relu1 (Activation)    (None, 16, 16, 64)        0
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 16, 16, 64)        36928
_________________________________________________________________
block2_bn2 (BatchNormalizati (None, 16, 16, 64)        256
_________________________________________________________________
block2_relu2 (Activation)    (None, 16, 16, 64)        0
_________________________________________________________________
block2_maxpool (MaxPooling2D (None, 8, 8, 64)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               2097664
_________________________________________________________________
dense_bn1 (BatchNormalizatio (None, 512)               2048
_________________________________________________________________
dense_relu1 (Activation)     (None, 512)               0
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130
=================================================================
Total params: 2,171,178
Trainable params: 2,169,770
Non-trainable params: 1,408
_________________________________________________________________
Epoch 1/200
391/391 [==============================] - 16s 40ms/step - loss: 1.5570 - acc: 0.4516 - val_loss: 1.3188 - val_acc: 0.5547
Epoch 2/200
391/391 [==============================] - 14s 35ms/step - loss: 1.1548 - acc: 0.5870 - val_loss: 1.4982 - val_acc: 0.5535
Epoch 3/200
391/391 [==============================] - 14s 36ms/step - loss: 1.0036 - acc: 0.6429 - val_loss: 1.0799 - val_acc: 0.6412
Epoch 4/200
391/391 [==============================] - 14s 36ms/step - loss: 0.9195 - acc: 0.6765 - val_loss: 1.0734 - val_acc: 0.6461
Epoch 5/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8580 - acc: 0.6988 - val_loss: 0.8321 - val_acc: 0.7158
Epoch 6/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8147 - acc: 0.7127 - val_loss: 1.0953 - val_acc: 0.6549
Epoch 7/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7796 - acc: 0.7247 - val_loss: 1.0431 - val_acc: 0.6820
Epoch 8/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7481 - acc: 0.7383 - val_loss: 0.6876 - val_acc: 0.7670
Epoch 9/200
391/391 [==============================] - 14s 35ms/step - loss: 0.7227 - acc: 0.7472 - val_loss: 0.6364 - val_acc: 0.7837
Epoch 10/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7049 - acc: 0.7541 - val_loss: 0.7113 - val_acc: 0.7580
Epoch 11/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6817 - acc: 0.7643 - val_loss: 0.7817 - val_acc: 0.7367
Epoch 12/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6709 - acc: 0.7651 - val_loss: 0.7277 - val_acc: 0.7546
Epoch 13/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6511 - acc: 0.7750 - val_loss: 0.8518 - val_acc: 0.7354
Epoch 14/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6357 - acc: 0.7796 - val_loss: 0.8052 - val_acc: 0.7435
Epoch 15/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6291 - acc: 0.7814 - val_loss: 0.5533 - val_acc: 0.8105
Epoch 16/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6139 - acc: 0.7868 - val_loss: 0.6044 - val_acc: 0.7910
Epoch 17/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6044 - acc: 0.7905 - val_loss: 0.7222 - val_acc: 0.7685
Epoch 18/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5963 - acc: 0.7917 - val_loss: 0.8040 - val_acc: 0.7443
Epoch 19/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5841 - acc: 0.7965 - val_loss: 0.6468 - val_acc: 0.7825
Epoch 20/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5782 - acc: 0.8000 - val_loss: 0.5541 - val_acc: 0.8128
Epoch 21/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5734 - acc: 0.8022 - val_loss: 0.6226 - val_acc: 0.7986
Epoch 22/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5593 - acc: 0.8062 - val_loss: 0.5888 - val_acc: 0.8083
Epoch 23/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5562 - acc: 0.8068 - val_loss: 0.6221 - val_acc: 0.7935
Epoch 24/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5481 - acc: 0.8117 - val_loss: 0.6074 - val_acc: 0.7997
Epoch 25/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5398 - acc: 0.8126 - val_loss: 0.6096 - val_acc: 0.7983
Epoch 26/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5331 - acc: 0.8167 - val_loss: 0.6440 - val_acc: 0.7837
Epoch 27/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5306 - acc: 0.8173 - val_loss: 0.7113 - val_acc: 0.7761
Epoch 28/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5254 - acc: 0.8191 - val_loss: 0.7358 - val_acc: 0.7713
Epoch 29/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5173 - acc: 0.8198 - val_loss: 0.5384 - val_acc: 0.8206
Epoch 30/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5122 - acc: 0.8252 - val_loss: 0.6142 - val_acc: 0.8002
Epoch 31/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5067 - acc: 0.8254 - val_loss: 0.5591 - val_acc: 0.8117
Epoch 32/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5083 - acc: 0.8248 - val_loss: 0.6291 - val_acc: 0.8002
Epoch 33/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5024 - acc: 0.8281 - val_loss: 0.5341 - val_acc: 0.8272
Epoch 34/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4957 - acc: 0.8294 - val_loss: 0.6389 - val_acc: 0.7921
Epoch 35/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4919 - acc: 0.8285 - val_loss: 0.6981 - val_acc: 0.7845
Epoch 36/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4928 - acc: 0.8307 - val_loss: 0.5713 - val_acc: 0.8139
Epoch 37/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4902 - acc: 0.8302 - val_loss: 0.5567 - val_acc: 0.8193
Epoch 38/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4778 - acc: 0.8354 - val_loss: 0.5271 - val_acc: 0.8283
Epoch 39/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4789 - acc: 0.8333 - val_loss: 0.5138 - val_acc: 0.8346
Epoch 40/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4771 - acc: 0.8346 - val_loss: 0.5536 - val_acc: 0.8231
Epoch 41/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4722 - acc: 0.8381 - val_loss: 0.5189 - val_acc: 0.8321
Epoch 42/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4642 - acc: 0.8381 - val_loss: 0.4599 - val_acc: 0.8475
Epoch 43/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4702 - acc: 0.8383 - val_loss: 0.4610 - val_acc: 0.8479
Epoch 44/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4590 - acc: 0.8415 - val_loss: 0.4503 - val_acc: 0.8494
Epoch 45/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4596 - acc: 0.8410 - val_loss: 0.4519 - val_acc: 0.8542
Epoch 46/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4550 - acc: 0.8423 - val_loss: 0.5858 - val_acc: 0.8155
Epoch 47/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4486 - acc: 0.8453 - val_loss: 0.5619 - val_acc: 0.8209
Epoch 48/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4528 - acc: 0.8438 - val_loss: 0.5099 - val_acc: 0.8376
Epoch 49/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4452 - acc: 0.8457 - val_loss: 0.4600 - val_acc: 0.8440
Epoch 50/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4452 - acc: 0.8469 - val_loss: 0.5863 - val_acc: 0.8138
Epoch 51/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4423 - acc: 0.8466 - val_loss: 0.5268 - val_acc: 0.8290
Epoch 52/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4439 - acc: 0.8478 - val_loss: 0.5085 - val_acc: 0.8367
Epoch 53/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4363 - acc: 0.8488 - val_loss: 0.5128 - val_acc: 0.8344
Epoch 54/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4363 - acc: 0.8489 - val_loss: 0.5360 - val_acc: 0.8277
Epoch 55/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4341 - acc: 0.8488 - val_loss: 0.5531 - val_acc: 0.8177

Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00030000000260770325.
Epoch 56/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4017 - acc: 0.8610 - val_loss: 0.4177 - val_acc: 0.8594
Epoch 57/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3846 - acc: 0.8653 - val_loss: 0.4008 - val_acc: 0.8655
Epoch 58/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3831 - acc: 0.8677 - val_loss: 0.4191 - val_acc: 0.8605
Epoch 59/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3785 - acc: 0.8695 - val_loss: 0.4387 - val_acc: 0.8530
Epoch 60/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3696 - acc: 0.8707 - val_loss: 0.4079 - val_acc: 0.8650
Epoch 61/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3711 - acc: 0.8720 - val_loss: 0.4129 - val_acc: 0.8632
Epoch 62/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3715 - acc: 0.8709 - val_loss: 0.4242 - val_acc: 0.8611
Epoch 63/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3716 - acc: 0.8706 - val_loss: 0.3944 - val_acc: 0.8692
Epoch 64/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3606 - acc: 0.8757 - val_loss: 0.4224 - val_acc: 0.8596
Epoch 65/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3645 - acc: 0.8760 - val_loss: 0.4034 - val_acc: 0.8649
Epoch 66/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3596 - acc: 0.8742 - val_loss: 0.4144 - val_acc: 0.8642
Epoch 67/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3633 - acc: 0.8752 - val_loss: 0.4167 - val_acc: 0.8633
Epoch 68/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3611 - acc: 0.8742 - val_loss: 0.4043 - val_acc: 0.8629
Epoch 69/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3579 - acc: 0.8753 - val_loss: 0.4037 - val_acc: 0.8665
Epoch 70/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3606 - acc: 0.8750 - val_loss: 0.4160 - val_acc: 0.8631
Epoch 71/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3562 - acc: 0.8757 - val_loss: 0.4017 - val_acc: 0.8667
Epoch 72/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3543 - acc: 0.8772 - val_loss: 0.3950 - val_acc: 0.8705
Epoch 73/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3577 - acc: 0.8760 - val_loss: 0.4091 - val_acc: 0.8648
Epoch 74/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3498 - acc: 0.8788 - val_loss: 0.3959 - val_acc: 0.8689
Epoch 75/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3508 - acc: 0.8790 - val_loss: 0.4253 - val_acc: 0.8605
Epoch 76/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3493 - acc: 0.8803 - val_loss: 0.4001 - val_acc: 0.8670
Epoch 77/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3485 - acc: 0.8779 - val_loss: 0.4107 - val_acc: 0.8655
Epoch 78/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3476 - acc: 0.8787 - val_loss: 0.4014 - val_acc: 0.8664
Epoch 79/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3437 - acc: 0.8806 - val_loss: 0.4019 - val_acc: 0.8672
Epoch 80/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3509 - acc: 0.8778 - val_loss: 0.4281 - val_acc: 0.8624
Epoch 81/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3464 - acc: 0.8799 - val_loss: 0.3931 - val_acc: 0.8708
Epoch 82/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3496 - acc: 0.8776 - val_loss: 0.4148 - val_acc: 0.8657
Epoch 83/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3443 - acc: 0.8813 - val_loss: 0.4154 - val_acc: 0.8649
Epoch 84/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3418 - acc: 0.8806 - val_loss: 0.4188 - val_acc: 0.8644
Epoch 85/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3417 - acc: 0.8796 - val_loss: 0.4068 - val_acc: 0.8669
Epoch 86/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3411 - acc: 0.8810 - val_loss: 0.4093 - val_acc: 0.8681
Epoch 87/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3415 - acc: 0.8817 - val_loss: 0.4077 - val_acc: 0.8657
Epoch 88/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3347 - acc: 0.8837 - val_loss: 0.3934 - val_acc: 0.8704
Epoch 89/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3464 - acc: 0.8790 - val_loss: 0.3870 - val_acc: 0.8716
Epoch 90/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3422 - acc: 0.8795 - val_loss: 0.4063 - val_acc: 0.8687
Epoch 91/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3441 - acc: 0.8794 - val_loss: 0.3902 - val_acc: 0.8717
Epoch 92/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3331 - acc: 0.8840 - val_loss: 0.4156 - val_acc: 0.8649
Epoch 93/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3323 - acc: 0.8851 - val_loss: 0.4194 - val_acc: 0.8643
Epoch 94/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3410 - acc: 0.8821 - val_loss: 0.3994 - val_acc: 0.8709
Epoch 95/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3428 - acc: 0.8814 - val_loss: 0.3935 - val_acc: 0.8714
Epoch 96/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3307 - acc: 0.8852 - val_loss: 0.4002 - val_acc: 0.8697
Epoch 97/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3388 - acc: 0.8805 - val_loss: 0.4084 - val_acc: 0.8667
Epoch 98/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3352 - acc: 0.8832 - val_loss: 0.4082 - val_acc: 0.8668
Epoch 99/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3342 - acc: 0.8835 - val_loss: 0.3928 - val_acc: 0.8703

Epoch 00099: ReduceLROnPlateau reducing learning rate to 3.000000142492354e-05.
Epoch 100/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3353 - acc: 0.8832 - val_loss: 0.3899 - val_acc: 0.8715
Epoch 101/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3317 - acc: 0.8842 - val_loss: 0.3889 - val_acc: 0.8721
Epoch 102/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3302 - acc: 0.8848 - val_loss: 0.3909 - val_acc: 0.8708
Epoch 103/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3305 - acc: 0.8864 - val_loss: 0.3921 - val_acc: 0.8714
Epoch 104/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3291 - acc: 0.8851 - val_loss: 0.3946 - val_acc: 0.8700
Epoch 105/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3323 - acc: 0.8843 - val_loss: 0.3912 - val_acc: 0.8712
Epoch 106/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3306 - acc: 0.8859 - val_loss: 0.3906 - val_acc: 0.8712
Epoch 107/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3278 - acc: 0.8860 - val_loss: 0.3938 - val_acc: 0.8704
Epoch 108/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3301 - acc: 0.8860 - val_loss: 0.3933 - val_acc: 0.8701
Epoch 109/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3292 - acc: 0.8869 - val_loss: 0.3928 - val_acc: 0.8711
Epoch 110/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3322 - acc: 0.8828 - val_loss: 0.3907 - val_acc: 0.8707
Epoch 111/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3267 - acc: 0.8862 - val_loss: 0.3943 - val_acc: 0.8702

Epoch 00111: ReduceLROnPlateau reducing learning rate to 3.000000106112566e-06.
Epoch 112/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3249 - acc: 0.8867 - val_loss: 0.3913 - val_acc: 0.8707
Epoch 113/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3299 - acc: 0.8845 - val_loss: 0.3935 - val_acc: 0.8703
Epoch 114/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3284 - acc: 0.8861 - val_loss: 0.3916 - val_acc: 0.8702
Epoch 115/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3274 - acc: 0.8857 - val_loss: 0.3912 - val_acc: 0.8708
Epoch 116/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3340 - acc: 0.8834 - val_loss: 0.3919 - val_acc: 0.8706