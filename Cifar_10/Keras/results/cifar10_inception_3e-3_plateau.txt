# Training parameters
LEARNING_RATE = 3e-3
DATA_AUGMENTATION = True
BATCH_SIZE = 128
EPOCHS = 200
KERNEL_INITIALIZER = 'he_normal'
es = keras.callbacks.EarlyStopping(monitor='val_acc', patience=15)
reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=1)



Using TensorFlow backend.
Using data augmentation.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0
__________________________________________________________________________________________________
stage1_conv1 (Conv2D)           (None, 32, 32, 32)   896         input_1[0][0]
__________________________________________________________________________________________________
stage1_conv2 (Conv2D)           (None, 32, 32, 32)   9248        stage1_conv1[0][0]
__________________________________________________________________________________________________
stage1_maxpool (MaxPooling2D)   (None, 16, 16, 32)   0           stage1_conv2[0][0]
__________________________________________________________________________________________________
stage2_1_1 (Conv2D)             (None, 16, 16, 64)   18496       stage1_maxpool[0][0]
__________________________________________________________________________________________________
stage2_3_3 (Conv2D)             (None, 16, 16, 128)  36992       stage1_maxpool[0][0]
__________________________________________________________________________________________________
stage2_5_5 (Conv2D)             (None, 16, 16, 32)   9248        stage1_maxpool[0][0]
__________________________________________________________________________________________________
stage2_pool_1_1 (Conv2D)        (None, 16, 16, 32)   9248        stage1_maxpool[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 16, 16, 256)  0           stage2_1_1[0][0]
                                                                 stage2_3_3[0][0]
                                                                 stage2_5_5[0][0]
                                                                 stage2_pool_1_1[0][0]
__________________________________________________________________________________________________
stage2_maxpool (MaxPooling2D)   (None, 8, 8, 256)    0           concatenate_1[0][0]
__________________________________________________________________________________________________
stage3_1_1 (Conv2D)             (None, 8, 8, 128)    295040      stage2_maxpool[0][0]
__________________________________________________________________________________________________
stage3_3_3 (Conv2D)             (None, 8, 8, 192)    442560      stage2_maxpool[0][0]
__________________________________________________________________________________________________
stage3_5_5 (Conv2D)             (None, 8, 8, 96)     221280      stage2_maxpool[0][0]
__________________________________________________________________________________________________
stage3_pool_1_1 (Conv2D)        (None, 8, 8, 64)     147520      stage2_maxpool[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 480)    0           stage3_1_1[0][0]
                                                                 stage3_3_3[0][0]
                                                                 stage3_5_5[0][0]
                                                                 stage3_pool_1_1[0][0]
__________________________________________________________________________________________________
stage3_maxpool (MaxPooling2D)   (None, 4, 4, 480)    0           concatenate_2[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 480)    0           stage3_maxpool[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 480)          0           average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 480)          0           flatten_1[0][0]
__________________________________________________________________________________________________
stage4_dense1 (Dense)           (None, 10)           4810        dropout_1[0][0]
==================================================================================================
Total params: 1,195,338
Trainable params: 1,195,338
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/200
391/391 [==============================] - 17s 43ms/step - loss: 1.7860 - acc: 0.3362 - val_loss: 1.4920 - val_acc: 0.4615
Epoch 2/200
391/391 [==============================] - 14s 36ms/step - loss: 1.4121 - acc: 0.4875 - val_loss: 1.1458 - val_acc: 0.5927
Epoch 3/200
391/391 [==============================] - 14s 36ms/step - loss: 1.2401 - acc: 0.5557 - val_loss: 1.0723 - val_acc: 0.6260
Epoch 4/200
391/391 [==============================] - 14s 36ms/step - loss: 1.1158 - acc: 0.6055 - val_loss: 0.9962 - val_acc: 0.6582
Epoch 5/200
391/391 [==============================] - 14s 36ms/step - loss: 1.0239 - acc: 0.6423 - val_loss: 0.9540 - val_acc: 0.6694
Epoch 6/200
391/391 [==============================] - 14s 35ms/step - loss: 0.9567 - acc: 0.6663 - val_loss: 0.8522 - val_acc: 0.7192
Epoch 7/200
391/391 [==============================] - 14s 36ms/step - loss: 0.9026 - acc: 0.6865 - val_loss: 0.9085 - val_acc: 0.7074
Epoch 8/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8681 - acc: 0.6983 - val_loss: 0.7827 - val_acc: 0.7368
Epoch 9/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8343 - acc: 0.7107 - val_loss: 0.8975 - val_acc: 0.7227
Epoch 10/200
391/391 [==============================] - 14s 36ms/step - loss: 0.8061 - acc: 0.7241 - val_loss: 0.7488 - val_acc: 0.7532
Epoch 11/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7882 - acc: 0.7285 - val_loss: 0.7817 - val_acc: 0.7500
Epoch 12/200
391/391 [==============================] - 14s 35ms/step - loss: 0.7705 - acc: 0.7359 - val_loss: 0.6947 - val_acc: 0.7722
Epoch 13/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7530 - acc: 0.7400 - val_loss: 0.6623 - val_acc: 0.7807
Epoch 14/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7371 - acc: 0.7486 - val_loss: 0.6708 - val_acc: 0.7788
Epoch 15/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7195 - acc: 0.7537 - val_loss: 0.6558 - val_acc: 0.7841
Epoch 16/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7157 - acc: 0.7537 - val_loss: 0.6168 - val_acc: 0.7943
Epoch 17/200
391/391 [==============================] - 14s 36ms/step - loss: 0.7081 - acc: 0.7576 - val_loss: 0.6579 - val_acc: 0.7832
Epoch 18/200
391/391 [==============================] - 14s 35ms/step - loss: 0.6885 - acc: 0.7645 - val_loss: 0.6820 - val_acc: 0.7777
Epoch 19/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6791 - acc: 0.7681 - val_loss: 0.6469 - val_acc: 0.7935
Epoch 20/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6797 - acc: 0.7673 - val_loss: 0.6135 - val_acc: 0.7961
Epoch 21/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6692 - acc: 0.7709 - val_loss: 0.6215 - val_acc: 0.7982
Epoch 22/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6566 - acc: 0.7776 - val_loss: 0.7797 - val_acc: 0.7691
Epoch 23/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6542 - acc: 0.7776 - val_loss: 0.6067 - val_acc: 0.8003
Epoch 24/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6460 - acc: 0.7779 - val_loss: 0.6266 - val_acc: 0.8066
Epoch 25/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6372 - acc: 0.7803 - val_loss: 0.6916 - val_acc: 0.7917
Epoch 26/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6344 - acc: 0.7844 - val_loss: 0.6329 - val_acc: 0.8007
Epoch 27/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6271 - acc: 0.7853 - val_loss: 0.6327 - val_acc: 0.8026
Epoch 28/200
391/391 [==============================] - 14s 35ms/step - loss: 0.6163 - acc: 0.7879 - val_loss: 0.6784 - val_acc: 0.7984
Epoch 29/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6079 - acc: 0.7915 - val_loss: 0.5217 - val_acc: 0.8279
Epoch 30/200
391/391 [==============================] - 14s 36ms/step - loss: 0.6103 - acc: 0.7918 - val_loss: 0.5630 - val_acc: 0.8168
Epoch 31/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5994 - acc: 0.7962 - val_loss: 0.5981 - val_acc: 0.8091
Epoch 32/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5990 - acc: 0.7945 - val_loss: 0.5512 - val_acc: 0.8213
Epoch 33/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5958 - acc: 0.7967 - val_loss: 0.6508 - val_acc: 0.8077
Epoch 34/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5937 - acc: 0.7971 - val_loss: 0.5428 - val_acc: 0.8285
Epoch 35/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5901 - acc: 0.7980 - val_loss: 0.6004 - val_acc: 0.8119
Epoch 36/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5820 - acc: 0.8002 - val_loss: 0.5903 - val_acc: 0.8225
Epoch 37/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5825 - acc: 0.7996 - val_loss: 0.5710 - val_acc: 0.8214
Epoch 38/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5858 - acc: 0.7993 - val_loss: 0.5799 - val_acc: 0.8204
Epoch 39/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5724 - acc: 0.8033 - val_loss: 0.5427 - val_acc: 0.8299
Epoch 40/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5761 - acc: 0.8028 - val_loss: 0.5357 - val_acc: 0.8275
Epoch 41/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5698 - acc: 0.8073 - val_loss: 0.5252 - val_acc: 0.8357
Epoch 42/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5612 - acc: 0.8095 - val_loss: 0.5980 - val_acc: 0.8150
Epoch 43/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5594 - acc: 0.8080 - val_loss: 0.5317 - val_acc: 0.8285
Epoch 44/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5586 - acc: 0.8074 - val_loss: 0.5065 - val_acc: 0.8382
Epoch 45/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5518 - acc: 0.8109 - val_loss: 0.5572 - val_acc: 0.8223
Epoch 46/200
391/391 [==============================] - 14s 35ms/step - loss: 0.5533 - acc: 0.8122 - val_loss: 0.6362 - val_acc: 0.8122
Epoch 47/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5518 - acc: 0.8127 - val_loss: 0.5376 - val_acc: 0.8292
Epoch 48/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5450 - acc: 0.8153 - val_loss: 0.6633 - val_acc: 0.8109
Epoch 49/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5470 - acc: 0.8131 - val_loss: 0.5340 - val_acc: 0.8318
Epoch 50/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5332 - acc: 0.8184 - val_loss: 0.5534 - val_acc: 0.8255
Epoch 51/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5375 - acc: 0.8151 - val_loss: 0.5344 - val_acc: 0.8339
Epoch 52/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5382 - acc: 0.8161 - val_loss: 0.5622 - val_acc: 0.8295
Epoch 53/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5346 - acc: 0.8193 - val_loss: 0.5653 - val_acc: 0.8226
Epoch 54/200
391/391 [==============================] - 14s 36ms/step - loss: 0.5326 - acc: 0.8180 - val_loss: 0.6033 - val_acc: 0.8246

Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00030000000260770325.
Epoch 55/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4700 - acc: 0.8376 - val_loss: 0.5335 - val_acc: 0.8396
Epoch 56/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4460 - acc: 0.8455 - val_loss: 0.4888 - val_acc: 0.8497
Epoch 57/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4404 - acc: 0.8479 - val_loss: 0.5032 - val_acc: 0.8480
Epoch 58/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4348 - acc: 0.8488 - val_loss: 0.4954 - val_acc: 0.8495
Epoch 59/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4288 - acc: 0.8522 - val_loss: 0.5023 - val_acc: 0.8470
Epoch 60/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4285 - acc: 0.8530 - val_loss: 0.4866 - val_acc: 0.8507
Epoch 61/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4184 - acc: 0.8554 - val_loss: 0.4960 - val_acc: 0.8486
Epoch 62/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4221 - acc: 0.8566 - val_loss: 0.4744 - val_acc: 0.8534
Epoch 63/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4176 - acc: 0.8570 - val_loss: 0.4807 - val_acc: 0.8530
Epoch 64/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4183 - acc: 0.8555 - val_loss: 0.5102 - val_acc: 0.8475
Epoch 65/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4097 - acc: 0.8584 - val_loss: 0.4973 - val_acc: 0.8518
Epoch 66/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4064 - acc: 0.8595 - val_loss: 0.4891 - val_acc: 0.8531
Epoch 67/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4090 - acc: 0.8586 - val_loss: 0.4743 - val_acc: 0.8571
Epoch 68/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4100 - acc: 0.8573 - val_loss: 0.4871 - val_acc: 0.8538
Epoch 69/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4055 - acc: 0.8619 - val_loss: 0.4859 - val_acc: 0.8545
Epoch 70/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4068 - acc: 0.8591 - val_loss: 0.5101 - val_acc: 0.8512
Epoch 71/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4038 - acc: 0.8605 - val_loss: 0.4855 - val_acc: 0.8545
Epoch 72/200
391/391 [==============================] - 14s 36ms/step - loss: 0.4071 - acc: 0.8593 - val_loss: 0.4785 - val_acc: 0.8550
Epoch 73/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4019 - acc: 0.8630 - val_loss: 0.4643 - val_acc: 0.8592
Epoch 74/200
391/391 [==============================] - 14s 35ms/step - loss: 0.4020 - acc: 0.8612 - val_loss: 0.4820 - val_acc: 0.8527
Epoch 75/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3997 - acc: 0.8612 - val_loss: 0.4547 - val_acc: 0.8615
Epoch 76/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3933 - acc: 0.8645 - val_loss: 0.4401 - val_acc: 0.8644
Epoch 77/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3965 - acc: 0.8626 - val_loss: 0.4708 - val_acc: 0.8564
Epoch 78/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3969 - acc: 0.8616 - val_loss: 0.4705 - val_acc: 0.8570
Epoch 79/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3883 - acc: 0.8666 - val_loss: 0.4716 - val_acc: 0.8586
Epoch 80/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3918 - acc: 0.8639 - val_loss: 0.4623 - val_acc: 0.8601
Epoch 81/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3877 - acc: 0.8649 - val_loss: 0.4704 - val_acc: 0.8588
Epoch 82/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3967 - acc: 0.8620 - val_loss: 0.4697 - val_acc: 0.8594
Epoch 83/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3887 - acc: 0.8671 - val_loss: 0.4752 - val_acc: 0.8619
Epoch 84/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3865 - acc: 0.8670 - val_loss: 0.4709 - val_acc: 0.8593
Epoch 85/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3837 - acc: 0.8666 - val_loss: 0.4782 - val_acc: 0.8592
Epoch 86/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3882 - acc: 0.8655 - val_loss: 0.4629 - val_acc: 0.8609

Epoch 00086: ReduceLROnPlateau reducing learning rate to 3.000000142492354e-05.
Epoch 87/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3755 - acc: 0.8693 - val_loss: 0.4673 - val_acc: 0.8608
Epoch 88/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3753 - acc: 0.8692 - val_loss: 0.4689 - val_acc: 0.8602
Epoch 89/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3775 - acc: 0.8691 - val_loss: 0.4654 - val_acc: 0.8599
Epoch 90/200
391/391 [==============================] - 14s 36ms/step - loss: 0.3763 - acc: 0.8684 - val_loss: 0.4702 - val_acc: 0.8604
Epoch 91/200
391/391 [==============================] - 14s 35ms/step - loss: 0.3760 - acc: 0.8673 - val_loss: 0.4762 - val_acc: 0.8602